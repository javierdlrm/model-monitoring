{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris ML Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import jobs, hdfs, serving, featurestore\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "import time, random\n",
    "import numpy as np\n",
    "\n",
    "FILE_NAME = 'model-monitoring-1.0-SNAPSHOT.jar'\n",
    "IRIS_RESOURCES_DIR_NAME = \"Resources/Iris/\"\n",
    "IRIS_RESOURCES_DIR = \"hdfs:///Projects/\" + hdfs.project_name() + \"/\" + IRIS_RESOURCES_DIR_NAME\n",
    "APP_PATH = IRIS_RESOURCES_DIR + FILE_NAME\n",
    "IRIS_MODEL_NAME=\"Iris\"\n",
    "IRIS_TRAIN_DATASET_NAME = \"iris_train_dataset\"\n",
    "IRIS_FG_NAME = \"iris_train_all_features\"\n",
    "\n",
    "# Structured Streaming\n",
    "JOB_NAME = 'iris_ml_monitoring_struct'\n",
    "CLASS_NAME = 'io.hops.ml.monitoring.examples.iris.IrisMLMonitoring'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark streaming job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_dyn_alloc_config(dyn_alloc_enabled=True, dyn_alloc_min_exec=1, dyn_alloc_max_exec=2, dyn_alloc_init_exec=1):\n",
    "    return { \"spark.dynamicAllocation.enabled\": dyn_alloc_enabled, \"spark.dynamicAllocation.minExecutors\": dyn_alloc_min_exec,\n",
    "              \"spark.dynamicAllocation.maxExecutors\": dyn_alloc_max_exec, \"spark.dynamicAllocation.initialExecutors\": dyn_alloc_init_exec }\n",
    "\n",
    "def get_spark_job_config(dyn_alloc_config, exec_instances=1, exec_gpus=0, exec_cores=1, exec_mem=2048, tf_num_ps=1, black_list_enabled=False):\n",
    "    config = { \"spark.executor.instances\": exec_instances, \"spark.executor.cores\": exec_cores, \"spark.executor.memory\": exec_mem,\n",
    "            \"spark.executor.gpus\": exec_gpus, \"spark.tensorflow.num.ps\": tf_num_ps, \"spark.blacklist.enabled\": black_list_enabled }\n",
    "    config.update(dyn_alloc_config)\n",
    "    return config\n",
    "\n",
    "def get_job_config(app_path, main_class, experiment_type=\"EXPERIMENT\", schedule=None, local_resources=[], dist_strategy=\"COLLECTIVE_ALL_REDUCE\", spark_config=None):\n",
    "    config = { 'appPath': app_path, 'mainClass': main_class, 'experimentType': experiment_type, 'distributionStrategy': dist_strategy, 'schedule': schedule, 'localResources': local_resources }\n",
    "    if spark_config:\n",
    "        base_spark_config = {'type': 'sparkJobConfiguration', 'amQueue': 'default', 'amMemory': 2048, 'amVCores': 1, 'jobType': 'SPARK',\n",
    "                             'mainClass': main_class}\n",
    "        config.update(base_spark_config)\n",
    "        config.update(spark_config)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job created with ID 1089"
     ]
    }
   ],
   "source": [
    "# generic job config\n",
    "spk_jb_dyn_alloc_conf = get_spark_dyn_alloc_config()\n",
    "spk_jb_config = get_spark_job_config(spk_jb_dyn_alloc_conf)\n",
    "job_config = get_job_config(APP_PATH, CLASS_NAME, spark_config=spk_jb_config)\n",
    "\n",
    "# check job existance\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "if executions:\n",
    "    print(\"Job '{}' already exists\".format(JOB_NAME))\n",
    "else:\n",
    "    # create streaming job\n",
    "    response = jobs.create_job(JOB_NAME, job_config)\n",
    "    if response and response['id']:\n",
    "        print(\"Job created with ID\", response['id'])\n",
    "    else:\n",
    "        print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job arguments:\n",
    "# NOTE: Avoid doubles\n",
    "job_timeout = 10*60 # 3m (seconds)\n",
    "window_duration = 4*1000 # 4s (milliseconds)\n",
    "slide_duration = 2*1000 # 2s (milliseconds)\n",
    "watermark_delay = 2*1000 # 2s (milliseconds)\n",
    "max_request_delay = 8 # seconds\n",
    "\n",
    "kfk_topic = serving.get_kafka_topic(IRIS_MODEL_NAME)\n",
    "job_args = \"{} {} {} {} {}\".format(kfk_topic, job_timeout, window_duration, slide_duration, watermark_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job execution started with ID 1194\n",
      "State: INITIALIZING - Args: 'Iris-inf7733 600 4000 2000 2000'"
     ]
    }
   ],
   "source": [
    "# check executions\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "job_execution_id = None\n",
    "if executions['count'] != 0:    \n",
    "    for item in executions['items']:\n",
    "        if item['finalStatus'] == \"UNDEFINED\":\n",
    "            job_execution_id = item['id']\n",
    "            print(\"Job '{}' already running with ID {}\".format(JOB_NAME, job_execution_id))\n",
    "            print(\"State: {} - Args: '{}'\".format(item['state'], item['args']))\n",
    "            break\n",
    "\n",
    "# start job if necessary\n",
    "if job_execution_id is None:    \n",
    "    response = jobs.start_job(JOB_NAME, job_args)\n",
    "    job_execution_id = response['id']\n",
    "    print(\"Job execution started with ID\", job_execution_id)\n",
    "    print(\"State: {} - Args: '{}'\".format(response['state'], response['args']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All executions: 1\n",
      "Job execution with ID 1194, State: INITIALIZING - Args: Iris-inf7733 600 4000 2000 2000"
     ]
    }
   ],
   "source": [
    "# see all executions\n",
    "response = jobs.get_executions(JOB_NAME, \"\")\n",
    "print(\"All executions:\", response['count'])\n",
    "for execution in response['items']:\n",
    "    print(\"Job execution with ID {}, State: {} - Args: {}\".format(execution['id'], execution['state'], execution['args']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Iris' already running"
     ]
    }
   ],
   "source": [
    "# verify model is served and running\n",
    "if serving.get_status(IRIS_MODEL_NAME) == 'Stopped':\n",
    "    serving.start(IRIS_MODEL_NAME)\n",
    "    time.sleep(10) # Let the serving startup correctly\n",
    "else:\n",
    "    print(\"Model '{}' already running\".format(IRIS_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check train data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(name, store_type):\n",
    "    if store_type == 'FEATUREGROUP':\n",
    "        return featurestore.get_featuregroup_statistics(name)\n",
    "    elif store_type == 'TRAINING_DATASET':\n",
    "        return featurestore.get_training_dataset_statistics(name)\n",
    "    raise Exception('Unknown store type')\n",
    "\n",
    "def get_clusters(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    cl_an = stats.cluster_analysis\n",
    "    clusters = cl_an.clusters\n",
    "    return [(cl.datapoint_name, cl.cluster) for cl in clusters]\n",
    "\n",
    "def get_correlation_matrix(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    features = []\n",
    "    correlations = []\n",
    "    row_feas = []\n",
    "    for fea_corr in stats.correlation_matrix.feature_correlations:\n",
    "        row_feas.append(fea_corr.feature_name)\n",
    "        col_corrs = []\n",
    "        for corr_val in fea_corr.correlation_values:\n",
    "            if len(correlations) == 0: features.append(corr_val.feature_name)\n",
    "            col_corrs.append(corr_val.correlation)\n",
    "        correlations.append(col_corrs)\n",
    "    row_idxs = list(map(lambda f: row_feas.index(f), features))\n",
    "    correlations = np.array(correlations)[row_idxs,:]\n",
    "    return features, correlations\n",
    "\n",
    "def get_descriptive_stats(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    \n",
    "    def merge_dicts(x,y):\n",
    "        x.update(y)\n",
    "        return x\n",
    "    \n",
    "    desc_stats = {}\n",
    "    for st in stats.descriptive_stats.descriptive_stats:\n",
    "        mv_dicts = list(map(lambda mv: {mv.metric_name: mv.value}, st.metric_values))\n",
    "        desc_stats[st.feature_name] = reduce(merge_dicts, mv_dicts)\n",
    "    return desc_stats\n",
    "\n",
    "def get_feature_histograms(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    fea_hist = {}\n",
    "    for fea_dist in stats.feature_histograms.feature_distributions:\n",
    "        fea_hist[fea_dist.feature_name] = list(map(lambda d: vars(d), fea_dist.frequency_distribution))\n",
    "    return fea_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats\n",
    "fg_stats = featurestore.get_featuregroup_statistics(IRIS_FG_NAME)\n",
    "td_stats = featurestore.get_training_dataset_statistics(IRIS_TRAIN_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters\n",
    "td_clusters = get_clusters(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_clusters = get_clusters(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "td_features, td_correlations = get_correlation_matrix(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_features, fg_correlations = get_correlation_matrix(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "td_desc_stats = get_descriptive_stats(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_desc_stats = get_descriptive_stats(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature histograms\n",
    "td_feature_hist = get_feature_histograms(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_feature_hist = get_feature_histograms(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics per feature\n",
    "feature_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'species': {'count': 120.0, 'mean': 1.0, 'stddev': 0.84016806, 'min': 0.0, 'max': 2.0}, 'petal_width': {'count': 120.0, 'mean': 1.1966667, 'stddev': 0.7820393, 'min': 0.1, 'max': 2.5}, 'petal_length': {'count': 120.0, 'mean': 3.7391667, 'stddev': 1.8221004, 'min': 1.0, 'max': 6.9}, 'sepal_width': {'count': 120.0, 'mean': 3.065, 'stddev': 0.42715594, 'min': 2.0, 'max': 4.4}, 'sepal_length': {'count': 120.0, 'mean': 5.845, 'stddev': 0.86857843, 'min': 4.4, 'max': 7.9}}"
     ]
    }
   ],
   "source": [
    "print(td_desc_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'species': [{'bin': '0.0', 'frequency': 42}, {'bin': '0.1', 'frequency': 0}, {'bin': '0.2', 'frequency': 0}, {'bin': '0.30000000000000004', 'frequency': 0}, {'bin': '0.4', 'frequency': 0}, {'bin': '0.5', 'frequency': 0}, {'bin': '0.6000000000000001', 'frequency': 0}, {'bin': '0.7000000000000001', 'frequency': 0}, {'bin': '0.8', 'frequency': 0}, {'bin': '0.9', 'frequency': 0}, {'bin': '1.0', 'frequency': 36}, {'bin': '1.1', 'frequency': 0}, {'bin': '1.2000000000000002', 'frequency': 0}, {'bin': '1.3', 'frequency': 0}, {'bin': '1.4000000000000001', 'frequency': 0}, {'bin': '1.5', 'frequency': 0}, {'bin': '1.6', 'frequency': 0}, {'bin': '1.7000000000000002', 'frequency': 0}, {'bin': '1.8', 'frequency': 0}, {'bin': '1.9000000000000001', 'frequency': 42}], 'petal_width': [{'bin': '0.10000000149011612', 'frequency': 27}, {'bin': '0.22000000141561032', 'frequency': 7}, {'bin': '0.34000000134110453', 'frequency': 7}, {'bin': '0.4600000012665987', 'frequency': 0}, {'bin': '0.5800000011920929', 'frequency': 1}, {'bin': '0.7000000011175871', 'frequency': 0}, {'bin': '0.8200000010430812', 'frequency': 0}, {'bin': '0.9400000009685755', 'frequency': 5}, {'bin': '1.0600000008940698', 'frequency': 3}, {'bin': '1.180000000819564', 'frequency': 12}, {'bin': '1.300000000745058', 'frequency': 7}, {'bin': '1.4200000006705522', 'frequency': 7}, {'bin': '1.5400000005960464', 'frequency': 3}, {'bin': '1.6600000005215405', 'frequency': 2}, {'bin': '1.7800000004470349', 'frequency': 15}, {'bin': '1.900000000372529', 'frequency': 4}, {'bin': '2.0200000002980234', 'frequency': 4}, {'bin': '2.1400000002235173', 'frequency': 3}, {'bin': '2.2600000001490117', 'frequency': 8}, {'bin': '2.3800000000745056', 'frequency': 5}], 'petal_length': [{'bin': '1.0', 'frequency': 3}, {'bin': '1.2950000047683716', 'frequency': 29}, {'bin': '1.5900000095367433', 'frequency': 9}, {'bin': '1.8850000143051147', 'frequency': 1}, {'bin': '2.1800000190734865', 'frequency': 0}, {'bin': '2.475000023841858', 'frequency': 0}, {'bin': '2.7700000286102293', 'frequency': 1}, {'bin': '3.065000033378601', 'frequency': 2}, {'bin': '3.3600000381469726', 'frequency': 2}, {'bin': '3.655000042915344', 'frequency': 4}, {'bin': '3.950000047683716', 'frequency': 7}, {'bin': '4.245000052452087', 'frequency': 10}, {'bin': '4.540000057220459', 'frequency': 10}, {'bin': '4.835000061988831', 'frequency': 13}, {'bin': '5.130000066757202', 'frequency': 5}, {'bin': '5.425000071525574', 'frequency': 10}, {'bin': '5.720000076293945', 'frequency': 5}, {'bin': '6.0150000810623165', 'frequency': 4}, {'bin': '6.310000085830688', 'frequency': 2}, {'bin': '6.60500009059906', 'frequency': 3}], 'sepal_width': [{'bin': '2.0', 'frequency': 1}, {'bin': '2.1200000047683716', 'frequency': 2}, {'bin': '2.240000009536743', 'frequency': 3}, {'bin': '2.3600000143051147', 'frequency': 3}, {'bin': '2.4800000190734863', 'frequency': 8}, {'bin': '2.600000023841858', 'frequency': 8}, {'bin': '2.7200000286102295', 'frequency': 12}, {'bin': '2.840000033378601', 'frequency': 7}, {'bin': '2.9600000381469727', 'frequency': 20}, {'bin': '3.0800000429153442', 'frequency': 10}, {'bin': '3.200000047683716', 'frequency': 17}, {'bin': '3.3200000524520874', 'frequency': 8}, {'bin': '3.440000057220459', 'frequency': 5}, {'bin': '3.5600000619888306', 'frequency': 3}, {'bin': '3.680000066757202', 'frequency': 9}, {'bin': '3.8000000715255737', 'frequency': 2}, {'bin': '3.9200000762939453', 'frequency': 1}, {'bin': '4.040000081062317', 'frequency': 0}, {'bin': '4.1600000858306885', 'frequency': 0}, {'bin': '4.28000009059906', 'frequency': 1}], 'sepal_length': [{'bin': '4.400000095367432', 'frequency': 4}, {'bin': '4.5750000953674315', 'frequency': 6}, {'bin': '4.750000095367431', 'frequency': 10}, {'bin': '4.925000095367432', 'frequency': 16}, {'bin': '5.100000095367432', 'frequency': 3}, {'bin': '5.275000095367432', 'frequency': 6}, {'bin': '5.4500000953674315', 'frequency': 7}, {'bin': '5.625000095367431', 'frequency': 6}, {'bin': '5.800000095367432', 'frequency': 8}, {'bin': '5.975000095367432', 'frequency': 9}, {'bin': '6.150000095367432', 'frequency': 9}, {'bin': '6.3250000953674315', 'frequency': 11}, {'bin': '6.500000095367431', 'frequency': 2}, {'bin': '6.675000095367432', 'frequency': 8}, {'bin': '6.850000095367431', 'frequency': 4}, {'bin': '7.025000095367432', 'frequency': 3}, {'bin': '7.2000000953674315', 'frequency': 1}, {'bin': '7.375000095367431', 'frequency': 1}, {'bin': '7.550000095367432', 'frequency': 5}, {'bin': '7.725000095367431', 'frequency': 1}]}"
     ]
    }
   ],
   "source": [
    "print(td_feature_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instance(verbose=False):\n",
    "    sl = round(np.random.uniform(3,9), 1)\n",
    "    sw = round(np.random.uniform(1,6), 1)\n",
    "    pl = round(np.random.uniform(0.1,8), 1)\n",
    "    pw = round(np.random.uniform(0.1,3.5), 1)\n",
    "    print(\"Request: \", [sl, sw, pl, pw])\n",
    "    return [sl, sw, pl, pw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(n_instances, signature_name, verbose=False):\n",
    "    instances = [generate_instance(verbose=verbose) for i in range(n_instances)]\n",
    "    data = { \"signature_name\": signature_name,\n",
    "             \"instances\": instances }\n",
    "    response = serving.make_inference_request(IRIS_MODEL_NAME, data)\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REQUESTS = 120\n",
    "\n",
    "time.sleep(15) # Let the job initiate completely\n",
    "\n",
    "for i in range(N_REQUESTS):\n",
    "    time.sleep(round(np.random.uniform(0, max_request_delay), 2))\n",
    "    # choose api randomly\n",
    "    signature = random.choice([tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, 'predict_instances'])\n",
    "    # choose nÂº instances randomly\n",
    "    n_instances = random.randint(1, 10)\n",
    "    # send request\n",
    "    preds = send_request(n_instances, signature, verbose=False)\n",
    "\n",
    "time.sleep(job_timeout) # wait until job finishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_STATS_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-stats-parquet/\"\n",
    "logs_stats_parquet_file = spark.read.parquet(LOGS_STATS_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_stats_parquet_file.createOrReplaceTempView(\"logs_stats_parquet_file\")\n",
    "desc_stats_df = spark.sql(\"SELECT window, feature, min, max, mean, avg, count, stddev FROM logs_stats_parquet_file ORDER BY window\")\n",
    "distr_stats_df = spark.sql(\"SELECT window, feature, distr FROM logs_stats_parquet_file ORDER BY window\")\n",
    "corr_stats_df = spark.sql(\"SELECT window, feature, corr FROM logs_stats_parquet_file ORDER BY window\")\n",
    "cov_stats_df = spark.sql(\"SELECT window, feature, cov FROM logs_stats_parquet_file ORDER BY window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(desc_stats_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distr_stats_df.show(1, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_stats_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cov_stats_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_OUTLIERS_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-outliers-parquet/\"\n",
    "logs_outliers_parquet_file = spark.read.parquet(LOGS_OUTLIERS_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_outliers_parquet_file.createOrReplaceTempView(\"logs_outliers_parquet_file\")\n",
    "outliers_df = spark.sql(\"SELECT * FROM logs_outliers_parquet_file ORDER BY window ASC, feature ASC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outliers_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_DRIFT_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-drift-parquet/\"\n",
    "logs_drift_parquet_file = spark.read.parquet(LOGS_DRIFT_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_drift_parquet_file.createOrReplaceTempView(\"logs_drift_parquet_file\")\n",
    "drift_df = spark.sql(\"SELECT * FROM logs_drift_parquet_file ORDER BY window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drift_df.show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, explode\n",
    "    \n",
    "distr_df = distr_stats_df \\\n",
    "    .withColumn(\"window_start\", unix_timestamp(col(\"window.start\")) * 1000) \\\n",
    "    .withColumn(\"window_end\", unix_timestamp(col(\"window.end\")) * 1000) \\\n",
    "    .drop(\"window\") \\\n",
    "    .select(col(\"window_start\"), col(\"window_end\"), col(\"feature\"), explode(\"distr\")) \\\n",
    "    .select(col(\"window_start\"), col(\"window_end\"), col(\"feature\").cast(\"string\"), col(\"key\").cast(\"float\"), col(\"value\").cast(\"float\"))\n",
    "distr_df.printSchema()\n",
    "\n",
    "proc_drift_df = drift_df \\\n",
    "    .withColumn(\"window_start\", unix_timestamp(col(\"window.start\")) * 1000) \\\n",
    "    .withColumn(\"window_end\", unix_timestamp(col(\"window.end\")) * 1000) \\\n",
    "    .drop(\"window\") \\\n",
    "    .select(col(\"window_start\"), col(\"window_end\"), col(\"feature\").cast(\"string\"), col(\"drift\").cast(\"string\"), col(\"value\").cast(\"float\"))\n",
    "\n",
    "proc_drift_df.printSchema()\n",
    "\n",
    "proc_drift_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "if 'distr_df' in locals() or 'distr_df' in globals(): del distr_df\n",
    "if 'proc_drift_df' in locals() or 'proc_drift_df' in globals(): del proc_drift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o distr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o proc_drift_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# fix column types after conversion from spark df\n",
    "distr_df.feature = distr_df.feature.astype(\"string\")\n",
    "proc_drift_df.feature = proc_drift_df.feature.astype(\"string\")\n",
    "proc_drift_df.drift = proc_drift_df.drift.astype(\"string\")\n",
    "proc_drift_df.value = proc_drift_df.value.values.astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Feature distributions\n",
    "\n",
    "f, axes = plt.subplots(2, 2, figsize=(20, 10))\n",
    "for ax, feature in zip(axes.flat, distr_df[\"feature\"].unique()):\n",
    "    feature_distr_df = distr_df[distr_df[\"feature\"]==feature]\n",
    "    plot = sns.barplot(x=round(feature_distr_df[\"key\"], 2), y=feature_distr_df[\"value\"], ax=ax)\n",
    "    plot.set_xticklabels(plot.get_xticklabels(), visible=True, rotation=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Drift\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(40, 10), sharex=True)\n",
    "for ax, drift in zip(axes.flat, proc_drift_df[\"drift\"].unique()):\n",
    "    features_drift_df = proc_drift_df[proc_drift_df[\"drift\"]==drift]\n",
    "    ax = sns.lineplot(x=features_drift_df[\"window_start\"], y=features_drift_df[\"value\"], hue=features_drift_df[\"feature\"], ax=ax)\n",
    "    xticks = ax.get_xticks()\n",
    "    ax.set_xticklabels([pd.to_datetime(tm, unit='ms').strftime('%Y-%m-%d\\n %H:%M:%S') for tm in xticks], rotation=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the model\n",
    "if serving.get_status(IRIS_MODEL_NAME) != 'Stopped':\n",
    "    serving.stop(IRIS_MODEL_NAME)\n",
    "    print(\"Model '{}' stopped\".format(IRIS_MODEL_NAME))\n",
    "else:\n",
    "    print(\"Model '{}' already stopped\".format(IRIS_MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**: Currently 'stop_job' method does not work. Url is not built properly, data is not serialized and header is not added.\n",
    "\n",
    "> The url \"/hopsworks-api/api/project/119/jobs/iris_ml_monitoring_dstream/executions/status\" is missing the execution number.\n",
    "\n",
    "> It should be \"/hopsworks-api/api/project/119/jobs/iris_ml_monitoring_dstream/executions/<EXECUTOR_NUMBER>/status\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING\n",
    "\n",
    "# stop job\n",
    "# response = jobs.stop_job(JOB_NAME)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/logicalclocks/hops-util-py/blob/7804a0d6734fe6e8a23c2598547316d40776e94c/hops/jobs.py#L96\n",
    "\n",
    "# Modification of stop_job method\n",
    "from hops import constants, util, hdfs\n",
    "from hops.exceptions import RestAPIError\n",
    "import json\n",
    "def stop_job(name, execution_id):\n",
    "    \"\"\"\n",
    "    Stop the current execution of the job.\n",
    "    Returns:\n",
    "        The job status.\n",
    "    \"\"\"\n",
    "    headers = {constants.HTTP_CONFIG.HTTP_CONTENT_TYPE: constants.HTTP_CONFIG.HTTP_APPLICATION_JSON}\n",
    "    method = constants.HTTP_CONFIG.HTTP_PUT\n",
    "    resource_url = constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_REST_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_PROJECT_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   hdfs.project_id() + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_JOBS_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   name + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_EXECUTIONS_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   str(execution_id) + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   \"status\"\n",
    "\n",
    "    status = {\"status\":\"stopped\"}\n",
    "    response = util.send_request(method, resource_url, data=json.dumps(status), headers=headers)\n",
    "    response_object = response.json()\n",
    "    if response.status_code >= 400:\n",
    "        error_code, error_msg, user_msg = util._parse_rest_error(response_object)\n",
    "        raise RestAPIError(\"Could not perform action on job's execution (url: {}), server response: \\n \"\n",
    "                           \"HTTP code: {}, HTTP reason: {}, error code: {}, error msg: {}, user msg: {}\".format(\n",
    "            resource_url, response.status_code, response.reason, error_code, error_msg, user_msg))\n",
    "    return response_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the job\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "for item in executions['items']:\n",
    "    if item['id'] == job_execution_id and item['finalStatus'] == 'UNDEFINED':\n",
    "        response = stop_job(JOB_NAME, job_execution_id)\n",
    "        print(\"JOB execution with ID {} stopped when: \\n - Duration: {} - Progress: {}\".format(job_execution_id, response['duration'], response['progress']))\n",
    "    else:\n",
    "        print(\"JOB execution with ID {} already stopped: \\n - Duration: {} - Progress: {} - Final status: {} - State: {}\".format(job_execution_id, item['duration'], item['progress'], item['finalStatus'], item['state']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}