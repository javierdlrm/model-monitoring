{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris ML Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import jobs, hdfs, serving, featurestore\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "import time, random\n",
    "import numpy as np\n",
    "\n",
    "FILE_NAME = 'model-monitoring-1.0-SNAPSHOT.jar'\n",
    "IRIS_RESOURCES_DIR_NAME = \"Resources/Iris/\"\n",
    "IRIS_RESOURCES_DIR = \"hdfs:///Projects/\" + hdfs.project_name() + \"/\" + IRIS_RESOURCES_DIR_NAME\n",
    "APP_PATH = IRIS_RESOURCES_DIR + FILE_NAME\n",
    "IRIS_MODEL_NAME=\"Iris\"\n",
    "IRIS_TRAIN_DATASET_NAME = \"iris_train_dataset\"\n",
    "IRIS_FG_NAME = \"iris_train_all_features\"\n",
    "\n",
    "# Structured Streaming\n",
    "STRUCT_JOB_NAME = 'iris_ml_monitoring_struct'\n",
    "STRUCT_CLASS_NAME = 'io.hops.monitoring.examples.IrisMLMonitoringStructured'\n",
    "\n",
    "# Direct Streaming\n",
    "DSTREAM_JOB_NAME = 'iris_ml_monitoring_dstream'\n",
    "DSTREAM_CLASS_NAME = 'io.hops.monitoring.examples.IrisMLMonitoringDStream'\n",
    "\n",
    "# Choose type of streaming job\n",
    "\n",
    "# JOB_NAME = DSTREAM_JOB_NAME\n",
    "# CLASS_NAME = DSTREAM_CLASS_NAME\n",
    "JOB_NAME = STRUCT_JOB_NAME\n",
    "CLASS_NAME = STRUCT_CLASS_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark streaming job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_dyn_alloc_config(dyn_alloc_enabled=True, dyn_alloc_min_exec=1, dyn_alloc_max_exec=2, dyn_alloc_init_exec=1):\n",
    "    return { \"spark.dynamicAllocation.enabled\": dyn_alloc_enabled, \"spark.dynamicAllocation.minExecutors\": dyn_alloc_min_exec,\n",
    "              \"spark.dynamicAllocation.maxExecutors\": dyn_alloc_max_exec, \"spark.dynamicAllocation.initialExecutors\": dyn_alloc_init_exec }\n",
    "\n",
    "def get_spark_job_config(dyn_alloc_config, exec_instances=1, exec_gpus=0, exec_cores=1, exec_mem=2048, tf_num_ps=1, black_list_enabled=False):\n",
    "    config = { \"spark.executor.instances\": exec_instances, \"spark.executor.cores\": exec_cores, \"spark.executor.memory\": exec_mem,\n",
    "            \"spark.executor.gpus\": exec_gpus, \"spark.tensorflow.num.ps\": tf_num_ps, \"spark.blacklist.enabled\": black_list_enabled }\n",
    "    config.update(dyn_alloc_config)\n",
    "    return config\n",
    "\n",
    "def get_job_config(app_path, main_class, experiment_type=\"EXPERIMENT\", schedule=None, local_resources=[], dist_strategy=\"COLLECTIVE_ALL_REDUCE\", spark_config=None):\n",
    "    config = { 'appPath': app_path, 'mainClass': main_class, 'experimentType': experiment_type, 'distributionStrategy': dist_strategy, 'schedule': schedule, 'localResources': local_resources }\n",
    "    if spark_config:\n",
    "        base_spark_config = {'type': 'sparkJobConfiguration', 'amQueue': 'default', 'amMemory': 2048, 'amVCores': 1, 'jobType': 'SPARK',\n",
    "                             'mainClass': main_class}\n",
    "        config.update(base_spark_config)\n",
    "        config.update(spark_config)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 'iris_ml_monitoring_struct' already exists"
     ]
    }
   ],
   "source": [
    "# generic job config\n",
    "spk_jb_dyn_alloc_conf = get_spark_dyn_alloc_config()\n",
    "spk_jb_config = get_spark_job_config(spk_jb_dyn_alloc_conf)\n",
    "job_config = get_job_config(APP_PATH, CLASS_NAME, spark_config=spk_jb_config)\n",
    "\n",
    "# check job existance\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "if executions:\n",
    "    print(\"Job '{}' already exists\".format(JOB_NAME))\n",
    "else:\n",
    "    # create streaming job\n",
    "    response = jobs.create_job(JOB_NAME, job_config)\n",
    "    if response and response['id']:\n",
    "        print(\"Job created with ID\", response['id'])\n",
    "    else:\n",
    "        print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start monitoring job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job arguments:\n",
    "# NOTE: Avoid doubles\n",
    "job_timeout = 2*60 # seconds\n",
    "window_duration = 6*1000 # 40s (milliseconds)\n",
    "slide_duration = 3*1000 # 5s (milliseconds)\n",
    "watermark_delay = 4*1000 # 20s (milliseconds)\n",
    "max_request_delay = 2 # seconds\n",
    "\n",
    "kfk_topic = serving.get_kafka_topic(IRIS_MODEL_NAME)\n",
    "job_args = \"{} {} {} {} {}\".format(kfk_topic, job_timeout, window_duration, slide_duration, watermark_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not perform action on job's execution (url: /hopsworks-api/api/project/119/jobs/iris_ml_monitoring_struct/executions), server response: \n",
      " HTTP code: 500, HTTP reason: Internal Server Error, error code: 120000, error msg: A generic error occurred., user msg: \n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/anaconda/envs/ml_monitoring/lib/python3.6/site-packages/hops/jobs.py\", line 91, in start_job\n",
      "    resource_url, response.status_code, response.reason, error_code, error_msg, user_msg))\n",
      "hops.exceptions.RestAPIError: Could not perform action on job's execution (url: /hopsworks-api/api/project/119/jobs/iris_ml_monitoring_struct/executions), server response: \n",
      " HTTP code: 500, HTTP reason: Internal Server Error, error code: 120000, error msg: A generic error occurred., user msg: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check executions\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "job_execution_id = None\n",
    "if executions['count'] != 0:    \n",
    "    for item in executions['items']:\n",
    "        if item['finalStatus'] == \"UNDEFINED\":\n",
    "            job_execution_id = item['id']\n",
    "            print(\"Job '{}' already running with ID {}\".format(JOB_NAME, job_execution_id))\n",
    "            print(\"State: {} - Args: '{}'\".format(item['state'], item['args']))\n",
    "            break\n",
    "\n",
    "# start job if necessary\n",
    "if job_execution_id is None:    \n",
    "    response = jobs.start_job(JOB_NAME, job_args)\n",
    "    job_execution_id = response['id']\n",
    "    print(\"Job execution started with ID\", job_execution_id)\n",
    "    print(\"State: {} - Args: '{}'\".format(response['state'], response['args']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All executions: 1\n",
      "Job execution with ID 280, State: FINISHED - Args: Iris-inf1958 120 6000 3000 4000"
     ]
    }
   ],
   "source": [
    "# see all executions\n",
    "response = jobs.get_executions(JOB_NAME, \"\")\n",
    "print(\"All executions:\", response['count'])\n",
    "for execution in response['items']:\n",
    "    print(\"Job execution with ID {}, State: {} - Args: {}\".format(execution['id'], execution['state'], execution['args']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Iris' already running"
     ]
    }
   ],
   "source": [
    "# verify model is served and running\n",
    "if serving.get_status(IRIS_MODEL_NAME) == 'Stopped':\n",
    "    serving.start(IRIS_MODEL_NAME)\n",
    "    time.sleep(10) # Let the serving startup correctly\n",
    "else:\n",
    "    print(\"Model '{}' already running\".format(IRIS_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check train data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(name, store_type):\n",
    "    if store_type == 'FEATUREGROUP':\n",
    "        return featurestore.get_featuregroup_statistics(name)\n",
    "    elif store_type == 'TRAINING_DATASET':\n",
    "        return featurestore.get_training_dataset_statistics(name)\n",
    "    raise Exception('Unknown store type')\n",
    "\n",
    "def get_clusters(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    cl_an = stats.cluster_analysis\n",
    "    clusters = cl_an.clusters\n",
    "    return [(cl.datapoint_name, cl.cluster) for cl in clusters]\n",
    "\n",
    "def get_correlation_matrix(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    features = []\n",
    "    correlations = []\n",
    "    row_feas = []\n",
    "    for fea_corr in stats.correlation_matrix.feature_correlations:\n",
    "        row_feas.append(fea_corr.feature_name)\n",
    "        col_corrs = []\n",
    "        for corr_val in fea_corr.correlation_values:\n",
    "            if len(correlations) == 0: features.append(corr_val.feature_name)\n",
    "            col_corrs.append(corr_val.correlation)\n",
    "        correlations.append(col_corrs)\n",
    "    row_idxs = list(map(lambda f: row_feas.index(f), features))\n",
    "    correlations = np.array(correlations)[row_idxs,:]\n",
    "    return features, correlations\n",
    "\n",
    "def get_descriptive_stats(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    \n",
    "    def merge_dicts(x,y):\n",
    "        x.update(y)\n",
    "        return x\n",
    "    \n",
    "    desc_stats = {}\n",
    "    for st in stats.descriptive_stats.descriptive_stats:\n",
    "        mv_dicts = list(map(lambda mv: {mv.metric_name: mv.value}, st.metric_values))\n",
    "        desc_stats[st.feature_name] = reduce(merge_dicts, mv_dicts)\n",
    "    return desc_stats\n",
    "\n",
    "def get_feature_histograms(name, store_type, stats=None):\n",
    "    stats = stats or get_stats(name, store_type)\n",
    "    fea_hist = {}\n",
    "    for fea_dist in stats.feature_histograms.feature_distributions:\n",
    "        fea_hist[fea_dist.feature_name] = list(map(lambda d: vars(d), fea_dist.frequency_distribution))\n",
    "    return fea_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats\n",
    "fg_stats = featurestore.get_featuregroup_statistics(IRIS_FG_NAME)\n",
    "td_stats = featurestore.get_training_dataset_statistics(IRIS_TRAIN_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters\n",
    "td_clusters = get_clusters(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_clusters = get_clusters(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "td_features, td_correlations = get_correlation_matrix(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_features, fg_correlations = get_correlation_matrix(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptive statistics\n",
    "td_desc_stats = get_descriptive_stats(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_desc_stats = get_descriptive_stats(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature histograms\n",
    "td_feature_hist = get_feature_histograms(IRIS_TRAIN_DATASET_NAME, 'TRAINING_DATASET', stats=td_stats)\n",
    "fg_feature_hist = get_feature_histograms(IRIS_FG_NAME, 'FEATUREGROUP', stats=fg_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics per feature\n",
    "feature_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'species': {'count': 120.0, 'mean': 1.0, 'stddev': 0.84016806, 'min': 0.0, 'max': 2.0}, 'petal_width': {'count': 120.0, 'mean': 1.1966667, 'stddev': 0.7820393, 'min': 0.1, 'max': 2.5}, 'petal_length': {'count': 120.0, 'mean': 3.7391667, 'stddev': 1.8221004, 'min': 1.0, 'max': 6.9}, 'sepal_width': {'count': 120.0, 'mean': 3.065, 'stddev': 0.42715594, 'min': 2.0, 'max': 4.4}, 'sepal_length': {'count': 120.0, 'mean': 5.845, 'stddev': 0.86857843, 'min': 4.4, 'max': 7.9}}"
     ]
    }
   ],
   "source": [
    "print(td_desc_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instance():\n",
    "    sl = round(np.random.uniform(3,9), 1)\n",
    "    sw = round(np.random.uniform(1,6), 1)\n",
    "    pl = round(np.random.uniform(0.1,8), 1)\n",
    "    pw = round(np.random.uniform(0.1,3.5), 1)\n",
    "    print(\"Request: \", [sl, sw, pl, pw])\n",
    "    return [sl, sw, pl, pw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_request(n_instances, signature_name):\n",
    "    instances = [generate_instance() for i in range(n_instances)]\n",
    "    data = { \"signature_name\": signature_name,\n",
    "             \"instances\": instances }\n",
    "    response = serving.make_inference_request(IRIS_MODEL_NAME, data)\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request:  [8.5, 3.7, 6.3, 2.1]\n",
      "Request:  [8.5, 4.5, 1.9, 3.2]\n",
      "Request:  [8.3, 1.5, 7.5, 3.1]\n",
      "Request:  [7.9, 2.8, 1.9, 3.3]\n",
      "Request:  [6.3, 1.2, 7.0, 0.5]\n",
      "Request:  [4.2, 5.1, 2.4, 2.0]\n",
      "Request:  [7.0, 4.6, 7.3, 0.4]\n",
      "Request:  [4.6, 2.8, 5.5, 1.4]\n",
      "Request:  [8.6, 2.6, 0.3, 2.3]\n",
      "Request:  [5.8, 5.7, 7.9, 1.5]\n",
      "Request:  [6.9, 2.8, 6.4, 0.9]\n",
      "Request:  [3.8, 5.7, 5.1, 1.5]\n",
      "Request:  [7.9, 3.8, 5.5, 1.9]\n",
      "Request:  [5.4, 3.5, 1.6, 0.6]\n",
      "Request:  [7.2, 4.0, 0.9, 2.2]\n",
      "Request:  [6.7, 1.3, 7.3, 2.3]\n",
      "Request:  [3.3, 2.7, 5.3, 0.5]\n",
      "Request:  [6.3, 1.7, 3.2, 0.4]\n",
      "Request:  [8.3, 5.2, 2.2, 1.8]\n",
      "Request:  [7.8, 4.7, 2.8, 2.7]\n",
      "Request:  [4.2, 1.2, 2.4, 2.3]\n",
      "Request:  [3.6, 5.2, 6.5, 2.0]\n",
      "Request:  [8.7, 2.6, 5.1, 3.4]\n",
      "Request:  [4.7, 3.8, 4.5, 2.0]\n",
      "Request:  [7.3, 4.4, 1.2, 1.9]\n",
      "Request:  [6.6, 3.5, 3.5, 1.9]\n",
      "Request:  [6.0, 2.9, 1.9, 3.4]\n",
      "Request:  [3.1, 4.4, 2.9, 1.1]\n",
      "Request:  [7.7, 3.8, 3.9, 3.3]\n",
      "Request:  [6.8, 2.2, 4.0, 2.0]\n",
      "Request:  [3.3, 4.3, 1.5, 1.3]\n",
      "Request:  [3.0, 2.4, 4.5, 1.9]\n",
      "Request:  [5.0, 3.2, 3.2, 0.5]\n",
      "Request:  [8.6, 3.5, 6.0, 1.1]\n",
      "Request:  [4.2, 6.0, 6.1, 2.0]\n",
      "Request:  [3.2, 3.1, 1.4, 1.0]\n",
      "Request:  [5.2, 3.0, 3.0, 1.7]\n",
      "Request:  [8.1, 5.6, 0.1, 1.4]\n",
      "Request:  [4.9, 4.3, 6.8, 3.4]\n",
      "Request:  [8.5, 2.2, 6.0, 0.7]\n",
      "Request:  [5.6, 4.5, 0.7, 1.7]\n",
      "Request:  [6.1, 5.8, 5.6, 0.4]\n",
      "Request:  [7.5, 1.9, 6.7, 0.2]\n",
      "Request:  [5.5, 4.6, 7.9, 0.4]\n",
      "Request:  [5.1, 3.8, 7.9, 0.2]\n",
      "Request:  [7.5, 4.6, 5.7, 0.5]\n",
      "Request:  [5.6, 4.2, 6.3, 1.9]\n",
      "Request:  [4.3, 2.2, 6.0, 0.2]\n",
      "Request:  [8.6, 2.7, 3.4, 1.4]\n",
      "Request:  [8.9, 3.7, 5.4, 1.5]\n",
      "Request:  [5.0, 4.6, 7.9, 2.0]\n",
      "Request:  [3.3, 5.6, 4.5, 2.3]\n",
      "Request:  [8.5, 5.5, 3.5, 1.0]\n",
      "Request:  [4.5, 2.5, 2.2, 1.1]\n",
      "Request:  [5.7, 1.3, 4.7, 1.6]\n",
      "Request:  [4.1, 2.6, 5.7, 3.4]\n",
      "Request:  [5.4, 2.6, 2.8, 2.6]\n",
      "Request:  [9.0, 2.8, 1.2, 0.8]\n",
      "Request:  [4.7, 3.8, 6.0, 2.8]\n",
      "Request:  [7.0, 4.0, 3.2, 0.6]\n",
      "Request:  [6.6, 4.4, 3.5, 1.7]\n",
      "Request:  [7.4, 2.9, 5.3, 3.3]\n",
      "Request:  [6.5, 3.5, 7.1, 2.5]\n",
      "Request:  [3.5, 5.5, 6.5, 3.4]\n",
      "Request:  [7.8, 2.5, 1.4, 0.5]"
     ]
    }
   ],
   "source": [
    "# start simulation\n",
    "N_REQUESTS = 12\n",
    "\n",
    "time.sleep(20) # Let the job initiate completely\n",
    "\n",
    "for i in range(N_REQUESTS):\n",
    "    time.sleep(round(np.random.uniform(0, max_request_delay), 1))\n",
    "    # choose api randomly\n",
    "    signature = random.choice([tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, 'predict_instances'])\n",
    "    # choose nÂº instances randomly\n",
    "    n_instances = random.randint(1, 10)\n",
    "    # send request\n",
    "    preds = send_request(n_instances, signature)\n",
    "#     print(\"Requests sended: {}\".format(i+1), end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_STATS_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-parquet/\"\n",
    "logs_stats_parquet_file = spark.read.parquet(LOGS_STATS_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_stats_parquet_file.createOrReplaceTempView(\"logs_stats_parquet_file\")\n",
    "stats_df = spark.sql(\"SELECT * FROM logs_stats_parquet_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------+-----------+------------+-----------+------+\n",
      "|window                                    |sepal_length|sepal_width|petal_length|petal_width|stat  |\n",
      "+------------------------------------------+------------+-----------+------------+-----------+------+\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|3.0         |0.2        |0.2         |1.3        |min   |\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|9.0         |3.4        |7.4         |5.6        |max   |\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|0.15789473  |0.08421053 |0.18947369  |0.1131579  |mean  |\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|5.7131586   |1.6157894  |3.352632    |3.5131578  |avg   |\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|38.0        |38.0       |38.0        |38.0       |count |\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|0.47459388  |0.035475567|0.18949154  |0.31019497 |stddev|\n",
      "|[2020-03-04 19:41:39, 2020-03-04 19:41:45]|217.10002   |61.399998  |127.40002   |133.5      |sum   |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|3.1         |0.2        |0.9         |1.8        |min   |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|8.8         |3.3        |7.8         |5.4        |max   |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|0.43846157  |0.23846152 |0.5307692   |0.2769231  |mean  |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|6.369231    |1.4923077  |4.0538464   |3.0384612  |avg   |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|13.0        |13.0       |13.0        |13.0       |count |\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|0.25092545  |0.5218358  |0.33308673  |0.32864544 |stddev|\n",
      "|[2020-03-04 14:59:30, 2020-03-04 14:59:36]|82.8        |19.4       |52.7        |39.499996  |sum   |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|3.7         |0.6        |2.6         |1.1        |min   |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|8.2         |2.9        |7.8         |5.3        |max   |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|0.45        |0.23000002 |0.52000004  |0.42000002 |mean  |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|5.8100004   |1.9099998  |5.53        |3.6299999  |avg   |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|10.0        |10.0       |10.0        |10.0       |count |\n",
      "|[2020-03-04 14:53:48, 2020-03-04 14:53:54]|0.70333344  |0.4366666  |0.2233332   |0.5233333  |stddev|\n",
      "+------------------------------------------+------------+-----------+------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "print(stats_df.show(20, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Path does not exist: hdfs://10.0.2.15:8020/Projects/ml_monitoring/Resources/Iris/Iris-inf1958-alerts-parquet/*.parquet;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 316, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Path does not exist: hdfs://10.0.2.15:8020/Projects/ml_monitoring/Resources/Iris/Iris-inf1958-alerts-parquet/*.parquet;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_ALERTS_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-alerts-parquet/\"\n",
    "logs_alerts_parquet_file = spark.read.parquet(LOGS_ALERTS_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'logs_alerts_parquet_file' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'logs_alerts_parquet_file' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_alerts_parquet_file.createOrReplaceTempView(\"logs_alerts_parquet_file\")\n",
    "alerts_df = spark.sql(\"SELECT * FROM logs_alerts_parquet_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'alerts_df' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'alerts_df' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(alerts_df.show(20, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WindowStreamResolver logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from hops import hdfs\n",
    "\n",
    "LOGS_WINDOWRESOLVER_DIR = IRIS_RESOURCES_DIR + kfk_topic + \"-window-resolver-parquet/\"\n",
    "logs_window_resolver_parquet_file = spark.read.parquet(LOGS_WINDOWRESOLVER_DIR + \"*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_window_resolver_parquet_file.createOrReplaceTempView(\"logs_window_resolver_parquet_file\")\n",
    "window_resolver_df = spark.sql(\"SELECT * FROM logs_window_resolver_parquet_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(window_resolver_df.show(20, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop served model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the model\n",
    "if serving.get_status(IRIS_MODEL_NAME) != 'Stopped':\n",
    "    serving.stop(IRIS_MODEL_NAME)\n",
    "    print(\"Model '{}' stopped\".format(IRIS_MODEL_NAME))\n",
    "else:\n",
    "    print(\"Model '{}' already stopped\".format(IRIS_MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop monitoring job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **WARNING**: Currently 'stop_job' method does not work. Url is not built properly, data is not serialized and header is not added.\n",
    "\n",
    "> The url \"/hopsworks-api/api/project/119/jobs/iris_ml_monitoring_dstream/executions/status\" is missing the execution number.\n",
    "\n",
    "> It should be \"/hopsworks-api/api/project/119/jobs/iris_ml_monitoring_dstream/executions/<EXECUTOR_NUMBER>/status\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING\n",
    "\n",
    "# stop job\n",
    "# response = jobs.stop_job(JOB_NAME)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/logicalclocks/hops-util-py/blob/7804a0d6734fe6e8a23c2598547316d40776e94c/hops/jobs.py#L96\n",
    "\n",
    "# Modification of stop_job method\n",
    "from hops import constants, util, hdfs\n",
    "from hops.exceptions import RestAPIError\n",
    "import json\n",
    "def stop_job(name, execution_id):\n",
    "    \"\"\"\n",
    "    Stop the current execution of the job.\n",
    "    Returns:\n",
    "        The job status.\n",
    "    \"\"\"\n",
    "    headers = {constants.HTTP_CONFIG.HTTP_CONTENT_TYPE: constants.HTTP_CONFIG.HTTP_APPLICATION_JSON}\n",
    "    method = constants.HTTP_CONFIG.HTTP_PUT\n",
    "    resource_url = constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_REST_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_PROJECT_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   hdfs.project_id() + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_JOBS_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   name + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   constants.REST_CONFIG.HOPSWORKS_EXECUTIONS_RESOURCE + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   str(execution_id) + constants.DELIMITERS.SLASH_DELIMITER + \\\n",
    "                   \"status\"\n",
    "\n",
    "    status = {\"status\":\"stopped\"}\n",
    "    response = util.send_request(method, resource_url, data=json.dumps(status), headers=headers)\n",
    "    response_object = response.json()\n",
    "    if response.status_code >= 400:\n",
    "        error_code, error_msg, user_msg = util._parse_rest_error(response_object)\n",
    "        raise RestAPIError(\"Could not perform action on job's execution (url: {}), server response: \\n \"\n",
    "                           \"HTTP code: {}, HTTP reason: {}, error code: {}, error msg: {}, user msg: {}\".format(\n",
    "            resource_url, response.status_code, response.reason, error_code, error_msg, user_msg))\n",
    "    return response_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the job\n",
    "executions = jobs.get_executions(JOB_NAME, \"\")\n",
    "for item in executions['items']:\n",
    "    if item['id'] == job_execution_id and item['finalStatus'] == 'UNDEFINED':\n",
    "        response = stop_job(JOB_NAME, job_execution_id)\n",
    "        print(\"JOB execution with ID {} stopped when: \\n - Duration: {} - Progress: {}\".format(job_execution_id, response['duration'], response['progress']))\n",
    "    else:\n",
    "        print(\"JOB execution with ID {} already stopped: \\n - Duration: {} - Progress: {} - Final status: {} - State: {}\".format(job_execution_id, item['duration'], item['progress'], item['finalStatus'], item['state']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}